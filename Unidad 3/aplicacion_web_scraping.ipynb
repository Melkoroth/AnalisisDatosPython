{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acceso a los datos de la web\n",
    "\n",
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El __Web Scraping__ (o Scraping) son un conjunto de técnicas que se utilizan para obtener de forma automática el contenido que hay en páginas web a través de su código HTML. \n",
    "* Es una opción cuando no hay API's para extraer datos de la web\n",
    "* Otros términos:\n",
    "    * \"Spydering the web\"\n",
    "    * \"Web crawling\"\n",
    "\n",
    "Las técnicas de Scraping se pueden enmarcar dentro del campo del Big Data en la primera fase de recolección de datos para su posterior almacenamiento, tratamiento y visualización.\n",
    "\n",
    "El uso de estas técnicas tienen como finalidad recopilar grandes cantidades de datos de diferentes páginas web cuyo uso posterior puede ser muy variado:\n",
    "\n",
    "  * homogenización de datos, \n",
    "  * tratamiento de contenido para la extracción de conocimiento, \n",
    "  * complementar datos en una web, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Realizar una petición__\n",
    "\n",
    "La página web con la que vamos a jugar es:\n",
    "\"https://es.wikipedia.org/wiki/Anexo:Municipios_de_la_Comunidad_de_Madrid\"\n",
    " que contiene datos de la lista de municipios de la Comunidad de Madrid.\n",
    " \n",
    "Para acceder al contenido de una página web usamos el protocolo HTTP Request/Response. \n",
    "\n",
    "A continuación adjunto una función capaz de hacer una conexión HTTP para acceder a una página web y extraer información.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://es.wikipedia.org/wiki/Anexo:Municipios_de_la_Comunidad_de_Madrid\"\n",
    "\n",
    "# Realizamos la petición HTTP a la web\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* La función __get__ del módulo __request__ de Python abre una conexión con el servidor donde se encuentra la url (parámetro de  __get__) y manda la petición.\n",
    "\n",
    "* La respuesta del servidor se almacena en la variable (u objeto) _response_.\n",
    "\n",
    "* A partir del objeto _response_, que almacena muchos datos relacionados con la petición HTTP (cabeceras, cookies, etc.) obtenemos el __status_code__ y el __HTML__ (como un string) de la web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code        # atributo status_code: nos indicará si la petición \n",
    "                            # ha tenido éxito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La petición ha ido bien\n"
     ]
    }
   ],
   "source": [
    "# Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "statusCode = response.status_code \n",
    "\n",
    "if statusCode == 200:\n",
    "    print('La petición ha ido bien')\n",
    "else:\n",
    "    print('Problemas con la petición...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del objeto __response__, que almacena muchos datos relacionados con la petición HTTP (cabeceras, cookies, etc.) obtenemos el __status_Code__ y el __HTML__ (como un string) de la web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlText = response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Extraer información__\n",
    "\n",
    "A partir del string __htmlText__ que representa el código interno de la pagina web, parseamos la web para extraer la información que nos interesa.\n",
    "\n",
    "Mecanismos:\n",
    "\n",
    "* Usar expresiones regulares para localizar cadenas: esto puede ser muy laborioso, y pesado ...\n",
    "    \n",
    "    \n",
    "    \n",
    "* Usar alguna de las librerías de Python que permiten parsear el texto HTML de forma cómoda. \n",
    "    * __BeautifullSoup__ \n",
    "    * __Scrapy__\n",
    "    \n",
    "    \n",
    "__BeautifulSoup__  nos aporta los métodos necesarios (y muy bien optimizados) para obtener el contenido que hay entre las etiquetas HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "soup = BeautifulSoup(htmlText, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperar los datos de la tabla\n",
    "\n",
    "Las tablas de HTML se definen con tres etiquetas: `<table>` para crear la tabla, `<tr>` para crear cada fila y `<td>` para crear cada columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "    <tr>\n",
       "      <th><strong>Curso</strong></th>\n",
       "      <th><strong>Horas</strong></th>\n",
       "    </tr>\n",
       " \n",
       "    <tr>\n",
       "      <td>CSS</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       " \n",
       "    <tr>\n",
       "      <td>HTML</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "s = \"\"\"\n",
    "<table>\n",
    "    <tr>\n",
    "      <th><strong>Curso</strong></th>\n",
    "      <th><strong>Horas</strong></th>\n",
    "    </tr>\n",
    " \n",
    "    <tr>\n",
    "      <td>CSS</td>\n",
    "      <td>20</td>\n",
    "    </tr>\n",
    " \n",
    "    <tr>\n",
    "      <td>HTML</td>\n",
    "      <td>20</td>\n",
    "    </tr>\n",
    "</table>\"\"\"\n",
    "HTML(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ejemplo:__ Queremos recuperar los datos de la tabla que aparece en la página  [Municipios Madrid](https://es.wikipedia.org/wiki/Anexo:Municipios_de_la_Comunidad_de_Madrid).\n",
    "\n",
    "En este caso utilizamos el método __find_all()__ que lo que hace es coger todos los fragmentos del HTML que correpondan a las etiquetas __tr__ y __td__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = {}\n",
    "# Obtenemos cada una de las filas de la tabla \n",
    "rows = soup.find_all('tr')\n",
    "for r in rows:\n",
    "    #Seleccionamos las celdas de la tabla (td)\n",
    "    celdas=r.find_all('td')\n",
    "    # ignoramos la primera celda, que no tiene elementos td sino th (ver HTML de la página web)\n",
    "    if len(celdas)>0:\n",
    "        # En lugar de un separador de miles, se ha usado un caracter parecido a un espacio en blanco\n",
    "        # por lo que en la celda de habitantes hay que eliminar todos los caracteres que no sean números\n",
    "        #content.append([celdas[0].string, ''.join(c for c in celdas[1].string if c.isdigit())])\n",
    "        content[celdas[0].string] = { 'población': int( ''.join(c for c in celdas[1].string if c.isdigit())),\n",
    "                                      'superficie': float(celdas[2].string.replace(',', '.') ),\n",
    "                                      'altitud': int( ''.join(c for c in celdas[6].string if c.isdigit())),}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Valdemorillo'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4830cdfdd833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Valdemorillo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Valdemorillo'"
     ]
    }
   ],
   "source": [
    "content['Valdemorillo']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 Ejercicio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcula la población total de todos los municipios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sol:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Ejercicio\n",
    "\n",
    "Obtener un listado de los municipios cuya altitud sea por encima de los 700 metros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sol:\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
